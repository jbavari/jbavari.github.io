<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: docker | Josh Bavari's Ramblings]]></title>
  <link href="http://jbavari.github.io/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://jbavari.github.io/"/>
  <updated>2019-03-02T17:47:20-07:00</updated>
  <id>http://jbavari.github.io/</id>
  <author>
    <name><![CDATA[Josh Bavari]]></name>
    <email><![CDATA[jbavari@gmail.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Exploring Best Practices with Docker for older libraries]]></title>
    <link href="http://jbavari.github.io/blog/2014/10/21/exploring-best-practices-with-docker-for-older-libraries/"/>
    <updated>2014-10-21T21:29:00-06:00</updated>
    <id>http://jbavari.github.io/blog/2014/10/21/exploring-best-practices-with-docker-for-older-libraries</id>
    <content type="html"><![CDATA[<p>I am not pretending to be an expert about what&rsquo;s in this post, but merely a talking point to learn upon.</p>

<h2>Problem: I need to reassemble an old C++ project with some old libraries and files that may not be around (or have disappeared already).</h2>

<p>First theres a big chunk of files that are used strictly for rendering a video, ~560MB. Some of which had since gone missing.</p>

<p>Then theres some old C++ libraries which a previous shell script was doing a <code>wget</code> request for, and the files are nowhere to be found.</p>

<p>Finally, there&rsquo;s the need to rebuild the image used to render the files.</p>

<p>Theres so many ways to attack this problem, I&rsquo;m just going to cover my approaches. I&rsquo;m open to new ones as well.</p>

<h1>Potential solutions for rendering files</h1>

<ul>
<li>store on AWS S3</li>
<li>put into git repo</li>
<li>store on server somewhere</li>
</ul>


<h4>Lets break down the pros / cons of these</h4>

<h2>Store on AWS S3</h2>

<h3>PROS:</h3>

<ul>
<li>quick to add</li>
<li>cheap to store</li>
</ul>


<h3>CONS:</h3>

<ul>
<li>can go missing (and did)</li>
</ul>


<h2>Put into git repo</h2>

<h3>PROS:</h3>

<ul>
<li>versioning control with notes (none before)</li>
<li>the files give a story in time</li>
<li>cheap or free</li>
</ul>


<h3>CONS:</h3>

<ul>
<li>slow to pull repo (duh)</li>
<li>storing binary files (derp)</li>
</ul>


<h2>Store on server somewhere</h2>

<h3>PROS:</h3>

<ul>
<li>cheap to store</li>
<li>fast to access (local network)</li>
</ul>


<h3>CONS:</h3>

<ul>
<li>can go missing (and did)</li>
<li>no story to the files</li>
</ul>


<h1>Potential solutions for server image</h1>

<ul>
<li>single shell script to run for setting up image</li>
<li>dockerfile to build up the image with <code>RUN</code> commands</li>
<li>dockerfile to execute the single shell script</li>
</ul>


<p>Some of the libraries this said project was depending on are no longer where they were from a previous shell script to set them all up. That means I have to do some kind of dependency management. Whether that be forking the libraries into a git repo I know will be solid, or copying the files somewhere I can trust, or more simply committing them to my own repo (560 MB or more.. ugh).</p>

<p>This is my thought process, not sure if its right:</p>

<p>If your aim is to have something fully repeatable and easy to run again, go with the docker solution.</p>

<p>If your aim is to just get it done quickly, go with the shell script.</p>

<p>However, I still can&rsquo;t decipher what the pro/cons of the dockerfile just running a single shell script.</p>

<p>Let&rsquo;s dive deeper into the pros and cons of each.</p>

<h2>Single shell script</h2>

<p>Steps:</p>

<ul>
<li>Create instance from Amazon AMI</li>
<li>create / test shell script</li>
<li>copy shell script to server</li>
<li>run shell script on server</li>
</ul>


<h3>PROS:</h3>

<ul>
<li>quick to run (once completed, overall time)</li>
<li>quick to tell you of errors</li>
<li>works on my machine</li>
</ul>


<h3>CONS:</h3>

<ul>
<li>not easily repeatable</li>
<li>may not work in another environment (things are assumed)</li>
<li>not always easy to debug</li>
</ul>


<h2>Dockerfile with RUN commmands</h2>

<p>Steps:</p>

<ul>
<li>install docker (if not already)</li>
<li>create Dockerfile with RUN commands</li>
<li>ADD dependencies to the docker container</li>
<li>docker build image</li>
<li>docker run image</li>
<li>bundle image to Amazon AMI</li>
<li>start instance</li>
<li>profit</li>
</ul>


<h3>PROS:</h3>

<ul>
<li>control the starting point environment</li>
<li>commands verified to work step by step</li>
<li>easily repeatable</li>
<li>quick to tell you of errors</li>
<li>fast after first run (cache)</li>
</ul>


<h3>CONS:</h3>

<ul>
<li>slow start up with downloads/updates/git clones/etc</li>
<li>costly for disk space</li>
<li>must install docker / boot2docker / etc</li>
</ul>


<h2>Dockerfile to execute single shell script</h2>

<p>Steps:</p>

<ul>
<li>install docker (if not already)</li>
<li>create image from dockerfile</li>
<li>run image</li>
<li>create / test shell script in image</li>
<li>modify dockerfile &ndash; ADD shell script created in previous step</li>
</ul>


<h3>PROS:</h3>

<ul>
<li>quick to test out your commands</li>
</ul>


<h3>CONS</h3>

<ul>
<li>harder to have the diffs between images when modifying shell script</li>
</ul>

]]></content>
  </entry>
  
</feed>
